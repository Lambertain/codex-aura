# üìã –§–∞–∑–∞ 2.1: Cloud Service & Premium Features

**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** 8-12 –Ω–µ–¥–µ–ª—å  
**–¶–µ–ª—å:** –ó–∞–ø—É—Å–∫ –æ–±–ª–∞—á–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞ —Å –º–æ–Ω–µ—Ç–∏–∑–∞—Ü–∏–µ–π  
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** Production-ready SaaS + –ø–µ—Ä–≤—ã–µ –ø–ª–∞—Ç—è—â–∏–µ –∫–ª–∏–µ–Ω—Ç—ã

### E1-1: Neo4j Docker Setup
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–ù–∞—Å—Ç—Ä–æ–∏—Ç—å Neo4j –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ production.

**–î–µ–π—Å—Ç–≤–∏—è:**
- [ ] docker-compose.yml —Å Neo4j
- [ ] –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö environments
- [ ] Health check —Å–∫—Ä–∏–ø—Ç
- [ ] Backup/restore –ø—Ä–æ—Ü–µ–¥—É—Ä—ã

**docker-compose.yml:**
```yaml
services:
  neo4j:
    image: neo4j:5.15-community
    ports:
      - "7474:7474"  # Browser
      - "7687:7687"  # Bolt
    environment:
      NEO4J_AUTH: neo4j/password
      NEO4J_PLUGINS: '["apoc"]'
    volumes:
      - neo4j_data:/data
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Neo4j –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ docker-compose
- [ ] APOC plugin —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
- [ ] Browser –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ :7474

---

### E1-2: Neo4j Python Driver Integration
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è neo4j-driver –≤ –ø—Ä–æ–µ–∫—Ç.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# src/codex_aura/storage/neo4j_client.py

from neo4j import GraphDatabase, AsyncGraphDatabase
from contextlib import asynccontextmanager

class Neo4jClient:
    def __init__(self, uri: str, user: str, password: str):
        self._driver = AsyncGraphDatabase.driver(uri, auth=(user, password))
    
    async def close(self):
        await self._driver.close()
    
    @asynccontextmanager
    async def session(self):
        async with self._driver.session() as session:
            yield session
    
    async def health_check(self) -> bool:
        async with self.session() as session:
            result = await session.run("RETURN 1")
            return await result.single() is not None
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Connection pool —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Async –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è
- [ ] Health check –ø—Ä–æ—Ö–æ–¥–∏—Ç

---

### E1-3: Graph Schema Design
**–û—Ü–µ–Ω–∫–∞:** 3h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–°–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å Cypher-—Å—Ö–µ–º—É –¥–ª—è –≥—Ä–∞—Ñ–∞ –∫–æ–¥–∞.

**–°—Ö–µ–º–∞:**
```cypher
// Node types
CREATE CONSTRAINT file_path IF NOT EXISTS 
FOR (f:File) REQUIRE f.path IS UNIQUE;

CREATE CONSTRAINT class_fqn IF NOT EXISTS
FOR (c:Class) REQUIRE c.fqn IS UNIQUE;

CREATE CONSTRAINT function_fqn IF NOT EXISTS
FOR (fn:Function) REQUIRE fn.fqn IS UNIQUE;

// Indexes for performance
CREATE INDEX file_repo IF NOT EXISTS FOR (f:File) ON (f.repo_id);
CREATE INDEX node_name IF NOT EXISTS FOR (n:Node) ON (n.name);

// Edge types
// (:File)-[:CONTAINS]->(:Class)
// (:File)-[:IMPORTS]->(:File)
// (:Class)-[:EXTENDS]->(:Class)
// (:Function)-[:CALLS]->(:Function)
// (:Class)-[:HAS_METHOD]->(:Function)
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Constraints —Å–æ–∑–¥–∞–Ω—ã
- [ ] Indexes –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
- [ ] –ú–∏–≥—Ä–∞—Ü–∏–∏ –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã

---

### E1-4: Graph Import from SQLite
**–û—Ü–µ–Ω–∫–∞:** 4h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–ú–∏–≥—Ä–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≥—Ä–∞—Ñ–æ–≤ –∏–∑ SQLite –≤ Neo4j.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
async def migrate_graph_to_neo4j(
    sqlite_graph: Graph,
    neo4j_client: Neo4jClient
) -> str:
    """Migrate graph from SQLite to Neo4j."""
    async with neo4j_client.session() as session:
        # Create nodes
        for node in sqlite_graph.nodes:
            await session.run("""
                MERGE (n:Node {fqn: $fqn})
                SET n += $properties
                SET n:$label
            """, fqn=node.fqn, properties=node.dict(), label=node.type)
        
        # Create edges
        for edge in sqlite_graph.edges:
            await session.run("""
                MATCH (a:Node {fqn: $source})
                MATCH (b:Node {fqn: $target})
                MERGE (a)-[r:$type]->(b)
                SET r += $properties
            """, source=edge.source, target=edge.target, 
                type=edge.type, properties=edge.dict())
    
    return neo4j_graph_id
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] –í—Å–µ nodes –º–∏–≥—Ä–∏—Ä—É—é—Ç
- [ ] –í—Å–µ edges –º–∏–≥—Ä–∏—Ä—É—é—Ç
- [ ] –°–≤–æ–π—Å—Ç–≤–∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è
- [ ] Batch import –¥–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤

---

### E1-5: Neo4j Query Layer
**–û—Ü–µ–Ω–∫–∞:** 3h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–∏–ø–∏—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ –≥—Ä–∞—Ñ—É.

**–ó–∞–ø—Ä–æ—Å—ã:**
```python
class GraphQueries:
    async def get_dependencies(self, fqn: str, depth: int = 2) -> list[Node]:
        """Get all dependencies up to N levels deep."""
        query = """
        MATCH path = (start:Node {fqn: $fqn})-[:IMPORTS|CALLS|EXTENDS*1..$depth]->(dep)
        RETURN DISTINCT dep, length(path) as distance
        ORDER BY distance
        """
        ...
    
    async def get_dependents(self, fqn: str, depth: int = 2) -> list[Node]:
        """Get all nodes that depend on this node."""
        query = """
        MATCH path = (dependent)-[:IMPORTS|CALLS|EXTENDS*1..$depth]->(target:Node {fqn: $fqn})
        RETURN DISTINCT dependent, length(path) as distance
        ORDER BY distance
        """
        ...
    
    async def shortest_path(self, source: str, target: str) -> list[Node]:
        """Find shortest path between two nodes."""
        query = """
        MATCH path = shortestPath(
            (a:Node {fqn: $source})-[*]-(b:Node {fqn: $target})
        )
        RETURN path
        """
        ...
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] get_dependencies —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] get_dependents —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] shortest_path —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–µ—à–∏—Ä—É—é—Ç—Å—è

---

### E1-6: Storage Backend Abstraction
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–ê–±—Å—Ç—Ä–∞–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–µ–∂–¥—É SQLite –∏ Neo4j.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
from abc import ABC, abstractmethod
from enum import Enum

class StorageBackend(str, Enum):
    SQLITE = "sqlite"
    NEO4J = "neo4j"

class GraphStorage(ABC):
    @abstractmethod
    async def save_graph(self, graph: Graph) -> str: ...
    
    @abstractmethod
    async def load_graph(self, graph_id: str) -> Graph: ...
    
    @abstractmethod
    async def query_dependencies(self, fqn: str, depth: int) -> list[Node]: ...

class SQLiteStorage(GraphStorage): ...
class Neo4jStorage(GraphStorage): ...

def get_storage(backend: StorageBackend) -> GraphStorage:
    if backend == StorageBackend.SQLITE:
        return SQLiteStorage()
    return Neo4jStorage()
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –µ–¥–∏–Ω—ã–π
- [ ] SQLite —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ fallback
- [ ] –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ env

---

## E2: üîç Semantic Search

> **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç E3:** –ù–µ—Ç ‚Äî –º–æ–∂–Ω–æ –Ω–∞—á–∏–Ω–∞—Ç—å —Å—Ä–∞–∑—É

### E2-1: Vector Database Setup (Qdrant)
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–ù–∞—Å—Ç—Ä–æ–∏—Ç—å Qdrant –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è embeddings.

**docker-compose.yml:**
```yaml
services:
  qdrant:
    image: qdrant/qdrant:v1.7.4
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Qdrant –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
- [ ] REST API –¥–æ—Å—Ç—É–ø–µ–Ω
- [ ] gRPC API –¥–æ—Å—Ç—É–ø–µ–Ω

---

### E2-2: Embedding Service
**–û—Ü–µ–Ω–∫–∞:** 4h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–°–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings –∫–æ–¥–∞.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# src/codex_aura/search/embeddings.py

from openai import AsyncOpenAI
import tiktoken

class EmbeddingService:
    def __init__(self, model: str = "text-embedding-3-small"):
        self.client = AsyncOpenAI()
        self.model = model
        self.tokenizer = tiktoken.encoding_for_model(model)
    
    async def embed_code(self, code: str) -> list[float]:
        """Generate embedding for code snippet."""
        # Truncate if too long
        tokens = self.tokenizer.encode(code)
        if len(tokens) > 8191:
            code = self.tokenizer.decode(tokens[:8191])
        
        response = await self.client.embeddings.create(
            model=self.model,
            input=code
        )
        return response.data[0].embedding
    
    async def embed_batch(self, codes: list[str]) -> list[list[float]]:
        """Batch embedding for efficiency."""
        response = await self.client.embeddings.create(
            model=self.model,
            input=codes
        )
        return [item.embedding for item in response.data]
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Single embedding —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Batch embedding —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Token limit handled
- [ ] Rate limiting

---

### E2-3: Code Chunking Strategy
**–û—Ü–µ–Ω–∫–∞:** 3h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –∫–æ–¥–∞ –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è embeddings.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class CodeChunker:
    """Split code into semantic chunks for embedding."""
    
    def chunk_file(self, file_content: str, file_path: str) -> list[CodeChunk]:
        """Chunk file into functions/classes/blocks."""
        tree = ast.parse(file_content)
        chunks = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                chunks.append(CodeChunk(
                    content=ast.get_source_segment(file_content, node),
                    type="function",
                    name=node.name,
                    file_path=file_path,
                    start_line=node.lineno,
                    end_line=node.end_lineno
                ))
            elif isinstance(node, ast.ClassDef):
                # Class docstring + method signatures
                chunks.append(CodeChunk(
                    content=self._extract_class_summary(node, file_content),
                    type="class",
                    name=node.name,
                    file_path=file_path,
                    start_line=node.lineno,
                    end_line=node.end_lineno
                ))
        
        return chunks
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Functions –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è
- [ ] Classes –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è
- [ ] Docstrings –≤–∫–ª—é—á–∞—é—Ç—Å—è
- [ ] Line numbers —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è

---

### E2-4: Qdrant Collection Management
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–°–æ–∑–¥–∞–Ω–∏–µ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º–∏ –≤ Qdrant.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

class VectorStore:
    def __init__(self, url: str = "http://localhost:6333"):
        self.client = QdrantClient(url=url)
    
    async def create_collection(self, repo_id: str):
        """Create collection for a repository."""
        self.client.create_collection(
            collection_name=f"repo_{repo_id}",
            vectors_config=VectorParams(
                size=1536,  # text-embedding-3-small
                distance=Distance.COSINE
            )
        )
    
    async def upsert_chunks(
        self, 
        repo_id: str, 
        chunks: list[CodeChunk],
        embeddings: list[list[float]]
    ):
        """Insert or update code chunks."""
        points = [
            PointStruct(
                id=chunk.id,
                vector=embedding,
                payload={
                    "content": chunk.content,
                    "type": chunk.type,
                    "file_path": chunk.file_path,
                    "name": chunk.name,
                    "start_line": chunk.start_line,
                    "end_line": chunk.end_line
                }
            )
            for chunk, embedding in zip(chunks, embeddings)
        ]
        self.client.upsert(collection_name=f"repo_{repo_id}", points=points)
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Collection —Å–æ–∑–¥–∞—ë—Ç—Å—è
- [ ] Points –≤—Å—Ç–∞–≤–ª—è—é—Ç—Å—è
- [ ] Metadata —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è

---

### E2-5: Semantic Search Query
**–û—Ü–µ–Ω–∫–∞:** 3h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–¥–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class SemanticSearch:
    def __init__(self, embedding_service: EmbeddingService, vector_store: VectorStore):
        self.embeddings = embedding_service
        self.vectors = vector_store
    
    async def search(
        self,
        repo_id: str,
        query: str,
        limit: int = 10,
        score_threshold: float = 0.7
    ) -> list[SearchResult]:
        """Search for relevant code chunks."""
        query_embedding = await self.embeddings.embed_code(query)
        
        results = self.vectors.client.search(
            collection_name=f"repo_{repo_id}",
            query_vector=query_embedding,
            limit=limit,
            score_threshold=score_threshold
        )
        
        return [
            SearchResult(
                chunk=CodeChunk(**hit.payload),
                score=hit.score
            )
            for hit in results
        ]
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] –ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Score threshold —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç
- [ ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω—ã

---

### E2-6: Hybrid Search (Graph + Vector)
**–û—Ü–µ–Ω–∫–∞:** 4h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ graph traversal –∏ semantic search.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class HybridSearch:
    """Combine graph structure with semantic similarity."""
    
    async def search(
        self,
        repo_id: str,
        task: str,
        entry_points: list[str],
        depth: int = 2
    ) -> list[RankedNode]:
        # 1. Get structurally relevant nodes from graph
        graph_nodes = await self.graph.get_dependencies(
            entry_points, depth=depth
        )
        
        # 2. Get semantically relevant chunks
        semantic_results = await self.semantic.search(
            repo_id, task, limit=50
        )
        
        # 3. Combine scores
        node_scores = {}
        for node in graph_nodes:
            node_scores[node.fqn] = {
                "graph_score": 1.0 / (node.distance + 1),  # closer = higher
                "semantic_score": 0.0
            }
        
        for result in semantic_results:
            fqn = result.chunk.fqn
            if fqn in node_scores:
                node_scores[fqn]["semantic_score"] = result.score
            else:
                node_scores[fqn] = {
                    "graph_score": 0.1,  # not in graph, low base
                    "semantic_score": result.score
                }
        
        # 4. Final ranking
        ranked = []
        for fqn, scores in node_scores.items():
            combined = (
                0.4 * scores["graph_score"] + 
                0.6 * scores["semantic_score"]
            )
            ranked.append(RankedNode(fqn=fqn, score=combined))
        
        return sorted(ranked, key=lambda x: x.score, reverse=True)
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Graph + Vector –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç—Å—è
- [ ] –í–µ—Å–∞ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ
- [ ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª—É—á—à–µ —á–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ

---

### E2-7: Search API Endpoint
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0

**–û–ø–∏—Å–∞–Ω–∏–µ:**
REST API –¥–ª—è semantic search.

**Endpoint:**
```http
POST /api/v1/search
Content-Type: application/json

{
  "repo_id": "repo_abc123",
  "query": "authentication JWT validation",
  "mode": "hybrid",  // "semantic" | "graph" | "hybrid"
  "limit": 20,
  "filters": {
    "file_types": [".py"],
    "paths": ["src/auth/**"]
  }
}
```

**Response:**
```json
{
  "results": [
    {
      "fqn": "src.auth.jwt.validate_token",
      "type": "function",
      "file_path": "src/auth/jwt.py",
      "score": 0.92,
      "snippet": "def validate_token(token: str) -> Claims:..."
    }
  ],
  "total": 15,
  "search_mode": "hybrid"
}
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] API —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Filters –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è
- [ ] Response format –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π

---

### E2-8: Index Rebuild Command
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P1

**–û–ø–∏—Å–∞–Ω–∏–µ:**
CLI –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∏ –∏–Ω–¥–µ–∫—Å–∞.

**–ö–æ–º–∞–Ω–¥–∞:**
```bash
codex-aura index rebuild --repo-id repo_abc123 --force
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] –ü–æ–ª–Ω—ã–π rebuild —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Progress –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è
- [ ] --force –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç —Å –Ω—É–ª—è

---

## E3: üîó Webhook Handler

> ‚ö†Ô∏è **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç E3-5:** Incremental Graph Update

### E3-1: Webhook Receiver Setup
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0 | **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å:** ‚ùå

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–ë–∞–∑–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–∏—ë–º–∞ webhooks.

**Endpoint:**
```python
@app.post("/webhooks/github/{repo_id}")
async def github_webhook(
    repo_id: str,
    request: Request,
    x_hub_signature_256: str = Header(...)
):
    payload = await request.body()
    
    # Verify signature
    if not verify_github_signature(payload, x_hub_signature_256):
        raise HTTPException(401, "Invalid signature")
    
    event = request.headers.get("X-GitHub-Event")
    data = await request.json()
    
    # Queue for processing
    await webhook_queue.enqueue(
        WebhookEvent(repo_id=repo_id, event=event, data=data)
    )
    
    return {"status": "queued"}
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Signature verification —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Events queued

---

### E3-2: GitHub Event Handlers
**–û—Ü–µ–Ω–∫–∞:** 3h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0 | **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å:** ‚ùå

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π.

**–°–æ–±—ã—Ç–∏—è:**
```python
class WebhookProcessor:
    handlers = {
        "push": handle_push,
        "pull_request": handle_pull_request,
        "create": handle_branch_create,
        "delete": handle_branch_delete
    }
    
    async def handle_push(self, repo_id: str, data: dict):
        """Handle push event - update graph."""
        commits = data["commits"]
        changed_files = set()
        
        for commit in commits:
            changed_files.update(commit["added"])
            changed_files.update(commit["modified"])
            changed_files.update(commit["removed"])
        
        # Trigger incremental update
        await self.graph_updater.update_files(repo_id, list(changed_files))
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Push events –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è
- [ ] PR events –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è
- [ ] Branch events –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è

---

### E3-3: GitLab Webhook Support
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P1 | **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å:** ‚ùå

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ GitLab webhooks.

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] GitLab signature verification
- [ ] Event mapping –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Ç–∏–ø–∞–º

---

### E3-4: Webhook Queue (Redis)
**–û—Ü–µ–Ω–∫–∞:** 2h | **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** P0 | **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å:** ‚ùå

**–û–ø–∏—Å–∞–Ω–∏–µ:**
–û—á–µ—Ä–µ–¥—å –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ webhooks.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
from arq import create_pool
from arq.connections import RedisSettings

redis_settings = RedisSettings(host="localhost", port=6379)

async def process_webhook(ctx, event: WebhookEvent):
    processor = WebhookProcessor()
    await processor.process(event)

class WorkerSettings:
    functions = [process_webhook]
    redis_settings = redis_settings
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏:**
- [ ] Queue —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] Retry logic –µ—Å—Ç—å
- [ ] Dead letter queue –¥–ª—è failed